{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LocalGPT\n",
        "이 프로젝트는 원래 privateGPT( https://github.com/imartinez/privateGPT )에서 영감을 받았습니다. 여기에 있는 대부분의 설명은 원본 privateGPT에서 영감을 받았습니다.\n",
        "\n",
        "이 모델에서는 GPT4ALL 모델을 Vicuna-7B 모델로 교체했으며 원본 privateGPT에서 사용된 LlamaEmbeddings 대신 InstructorEmbeddings를 사용하고 있습니다. 임베딩과 LLM은 모두 CPU 대신 GPU에서 실행됩니다. GPU가 없는 경우 CPU도 지원합니다(지침은 아래 참조).\n",
        "\n",
        "LLM의 기능을 사용하여 인터넷 연결 없이 문서에 질문하십시오. 100% 비공개, 어떠한 데이터도 실행 환경을 벗어나지 않습니다. 인터넷 연결 없이 문서를 수집하고 질문할 수 있습니다!\n",
        "\n",
        "LangChain 및 Vicuna-7B 와 InstructorEmbeddings 로 구축"
      ],
      "metadata": {
        "id": "vizKqqBXgmr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/PromtEngineer/localGPT.git"
      ],
      "metadata": {
        "id": "_yVW2eeXgFZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "eH3GT1efgCxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cC3p0-jAgERz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "자신의 데이터 세트를 수집하기 위한 지침\n",
        "모든 .txt, .pdf 또는 .csv 파일을 load_documents() 함수의 SOURCE_DOCUMENTS 디렉토리에 넣고 docs_path를 source_documents 디렉토리의 절대 경로로 바꿉니다."
      ],
      "metadata": {
        "id": "OXLXIfuQgTf9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ingtest.py\n"
      ],
      "metadata": {
        "id": "C2PaR8r3gJe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "python ingest.py"
      ],
      "metadata": {
        "id": "JiXKNjQMgEUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "로컬 vectorstore를 포함하는 인덱스를 생성합니다. 문서의 크기에 따라 시간이 걸립니다. 원하는 만큼 문서를 수집할 수 있으며 모두 로컬 임베딩 데이터베이스에 축적됩니다. 빈 데이터베이스에서 시작하려면 index.\n",
        "\n",
        "참고: 처음 실행하면 임베딩 모델을 다운로드해야 하므로 다운로드하는 데 시간이 걸립니다. 후속 실행에서는 데이터가 로컬 환경을 떠나지 않으며 인터넷 연결 없이 실행할 수 있습니다."
      ],
      "metadata": {
        "id": "DGe2bixggj-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Local 실행"
      ],
      "metadata": {
        "id": "tTrR7T9kgbla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_localGPT.py"
      ],
      "metadata": {
        "id": "2CBD2OFzgetq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OTJ-oplXge0G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}